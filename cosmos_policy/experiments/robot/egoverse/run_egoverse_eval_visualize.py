# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
run_egoverse_eval_visualize.py

Visualizes model predictions on EgoVerse training dataset episodes.

Produces one MP4 per episode with the same 3-panel (+ optional wrist row)
layout as egomimic's validation videos
(logs/.../videos/epoch_N/EVA_BIMANUAL/validation_video_0.mp4):

  Row 1:  [Current + actions (blue)]  [GT future (green)]  [WM pred (red)]
  Row 2:  [Wrist current (blue)]      [GT future wrist (green)]  [WM pred wrist (red)]

Actions are overlaid on the "Current" panel using egomimic's draw_actions:
  - Purple: predicted action chunk
  - Green:  ground-truth action chunk

Usage:
    python -m cosmos_policy.experiments.robot.egoverse.run_egoverse_eval_visualize \
        --config cosmos_predict2_2b_480p_egoverse \
        --ckpt_path /path/to/checkpoint.pt \
        --config_file cosmos_policy/config/config.py \
        --num_denoising_steps 5 \
        --save_video_path ./visualizations \
        --max_episodes 3
"""

import logging
import os
from collections import defaultdict
from dataclasses import dataclass
from typing import Optional

import draccus
import imageio
import numpy as np
import torch
import tqdm
from PIL import Image as PILImage, ImageDraw as PILDraw

from cosmos_policy._src.imaginaire.lazy_config import instantiate
from cosmos_policy.experiments.robot.cosmos_utils import (
    DEVICE,
    extract_action_chunk_from_latent_sequence,
    get_model,
)
from cosmos_policy.experiments.robot.robot_utils import log_message, setup_logging
from cosmos_policy.utils.utils import set_seed_everywhere

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

# Cosmos temporal compression factor (WAN 2.1 tokenizer)
TCF = 4


# ---------------------------------------------------------------------------
# CLI config
# ---------------------------------------------------------------------------

@dataclass
class PolicyEvalVisualizeConfig:
    # fmt: off
    # ── Model ─────────────────────────────────────────────────────────────
    config: str = "cosmos_predict2_2b_480p_egoverse"   # experiment name
    ckpt_path: str = ""                                 # local path or HF repo
    config_file: str = "cosmos_policy/config/config.py"

    # ── Inference ─────────────────────────────────────────────────────────
    num_denoising_steps: int = 5                        # diffusion steps
    chunk_size: int = 25                                # must match dataset

    # ── Output ────────────────────────────────────────────────────────────
    save_video_path: str = "./visualizations"
    video_fps: int = 10                                 # output video fps
    panel_size: int = 224                               # H=W of each panel

    # ── Episode selection ──────────────────────────────────────────────────
    episode_indices: Optional[str] = None   # comma-separated, e.g. "0,1,2"; None=all
    max_episodes: Optional[int] = None      # cap total episodes; None=no cap
    n: Optional[int] = None                 # max frames to visualize per episode; None=all

    # ── Logging ───────────────────────────────────────────────────────────
    local_log_dir: str = "./experiments/logs"
    run_id_note: Optional[str] = None
    seed: int = 7
    deterministic: bool = True
    # fmt: on


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _to_uint8(img: np.ndarray) -> np.ndarray:
    """Ensure (H,W,3) uint8, converting from float if needed."""
    if img is None:
        return None
    img = np.asarray(img)
    if img.dtype != np.uint8:
        img = (img * 255).astype(np.uint8) if img.max() <= 1.0 else img.astype(np.uint8)
    if img.ndim == 3 and img.shape[0] == 3:
        img = img.transpose(1, 2, 0)
    return img


def _resize(img: np.ndarray, h: int, w: int) -> np.ndarray:
    """Resize to (h, w, 3) uint8 using PIL LANCZOS."""
    arr = np.ascontiguousarray(img).astype(np.uint8)
    pil = PILImage.fromarray(arr)
    if pil.mode != "RGB":
        pil = pil.convert("RGB")
    return np.array(pil.resize((w, h), PILImage.Resampling.LANCZOS))


def _labeled(img: np.ndarray, text: str, color=(255, 255, 255)) -> np.ndarray:
    """Add a 18-px black header strip with colored text — matches egomimic."""
    arr = np.ascontiguousarray(img).astype(np.uint8)
    pil = PILImage.fromarray(arr)
    if pil.mode != "RGB":
        pil = pil.convert("RGB")
    draw = PILDraw.Draw(pil)
    draw.rectangle([0, 0, pil.width, 18], fill=(0, 0, 0))
    draw.text((3, 2), text, fill=color)
    return np.array(pil)


def _scale_intrinsics(intrinsics: np.ndarray, img_h: int, img_w: int) -> np.ndarray:
    """
    Scale a (3, 4) camera intrinsics matrix so that the principal point and
    focal lengths match the actual image size.

    The ARIA reference intrinsics were calibrated for ~(2*cx) × (2*cy) resolution.
    We compute per-axis scale factors from the principal point coordinates.
    """
    K = np.array(intrinsics, dtype=np.float64).copy()
    ref_w = K[0, 2] * 2.0  # cx * 2 ≈ original width
    ref_h = K[1, 2] * 2.0  # cy * 2 ≈ original height
    if ref_w > 0:
        K[0, :] *= img_w / ref_w  # scale fx, cx (and the zero column)
    if ref_h > 0:
        K[1, :] *= img_h / ref_h  # scale fy, cy
    return K


def _extract_xyz(x: np.ndarray):
    """
    Extract xyz + rot from action array. Mirrors CosmosPolicy._extract_xyz.
    Supports 6, 7, 12, 14-dim actions.
    """
    d = x.shape[-1]
    if d == 6:
        return x[..., :3], x[..., 3:6]
    elif d == 7:
        return x[..., :3], x[..., 3:6]
    elif d == 12:
        return (np.concatenate([x[..., :3], x[..., 6:9]], axis=-1),
                np.concatenate([x[..., 3:6], x[..., 9:12]], axis=-1))
    elif d == 14:
        return (np.concatenate([x[..., :3], x[..., 7:10]], axis=-1),
                np.concatenate([x[..., 3:6], x[..., 10:13]], axis=-1))
    else:
        return x[..., :min(3, d)], x[..., min(3, d):min(6, d)]


def _batch_sample(
    sample: dict,
    device: torch.device,
    model_dtype: Optional[torch.dtype] = None,
) -> dict:
    """
    Add batch dim (B=1), move tensors to device, and (optionally) cast
    floating-point tensors to the model's dtype (e.g., bfloat16).
    """
    batched = {}
    for k, v in sample.items():
        if isinstance(v, torch.Tensor):
            t = v.unsqueeze(0).to(device)
            if model_dtype is not None and torch.is_floating_point(t):
                t = t.to(model_dtype)
            batched[k] = t
        elif isinstance(v, (int, float, np.integer, np.floating)):
            # Only cast floats to model_dtype; keep ints as default
            if isinstance(v, (float, np.floating)) and model_dtype is not None:
                batched[k] = torch.tensor([v], device=device, dtype=model_dtype)
            else:
                batched[k] = torch.tensor([v], device=device)
        elif isinstance(v, np.ndarray):
            t = torch.from_numpy(v).unsqueeze(0).to(device)
            if model_dtype is not None and torch.is_floating_point(t):
                t = t.to(model_dtype)
            batched[k] = t
        else:
            batched[k] = v  # str, None, etc.
    return batched


# ---------------------------------------------------------------------------
# Core inference + visualization (ported from egomimic CosmosPolicy)
# ---------------------------------------------------------------------------

def decode_wm_future_images(
    model,
    generated_latent: torch.Tensor,
    orig_clean_latent_frames: torch.Tensor,
    cosmos_batch: dict,
) -> Optional[dict]:
    """
    Decode WM-predicted future images from the diffusion output latent.
    Exact port of egomimic CosmosPolicy._decode_wm_future_images.

    Returns dict with keys (all (B, H, W, 3) uint8 numpy):
        future_image_pred, future_image_gt,
        future_wrist_image_pred (optional), future_wrist_image_gt (optional)
    or None if future_image_latent_idx == -1.
    """
    def _idx(key):
        v = cosmos_batch.get(key, -1)
        return int(v.flatten()[0].item()) if isinstance(v, torch.Tensor) else int(v)

    future_image_latent_idx       = _idx("future_image_latent_idx")
    future_wrist_image_latent_idx = _idx("future_wrist_image_latent_idx")
    if future_image_latent_idx == -1:
        return None

    # Undo latent injection on non-image slots to avoid VAE artifacts
    cleaned = generated_latent.clone()
    for key in ("current_proprio_latent_idx", "action_latent_idx",
                "future_proprio_latent_idx", "value_latent_idx"):
        idx = _idx(key)
        if idx != -1:
            cleaned[:, :, idx] = orig_clean_latent_frames[:, :, idx]

    # Decode: (B, 3, T_raw, H, W) in [-1, 1]
    with torch.no_grad():
        decoded = model.decode(cleaned.float())
    decoded_u8 = (
        ((decoded + 1.0) * 127.5).clamp(0, 255)
        .permute(0, 2, 3, 4, 1).contiguous().to(torch.uint8).cpu().numpy()
    )  # (B, T_raw, H, W, 3)

    T_raw  = decoded_u8.shape[1]
    result = {}

    pred_idx = max(0, min((future_image_latent_idx - 1) * TCF + 1, T_raw - 1))
    result["future_image_pred"] = np.ascontiguousarray(decoded_u8[:, pred_idx])

    if future_wrist_image_latent_idx != -1:
        wi = max(0, min((future_wrist_image_latent_idx - 1) * TCF + 1, T_raw - 1))
        result["future_wrist_image_pred"] = np.ascontiguousarray(decoded_u8[:, wi])

    return result


def build_visualization_frame(
    current_primary: np.ndarray,
    gt_future_primary: np.ndarray,
    wm_future_primary: np.ndarray,
    gt_actions: np.ndarray,
    pred_actions: np.ndarray,
    camera_transforms,
    panel_size: int,
    current_left_wrist: Optional[np.ndarray] = None,
    gt_future_left_wrist: Optional[np.ndarray] = None,
    wm_future_left_wrist: Optional[np.ndarray] = None,
) -> np.ndarray:
    """
    Build one video frame in the exact egomimic visualize_preds layout:

      Row 1: [Current + actions (blue)] [GT future (green)] [WM pred (red)]
      Row 2: [Wrist current (blue)]     [GT future wrist (green)] [WM pred wrist (red)]

    Actions are drawn with egomimic's draw_actions (Purples=pred, Greens=GT).
    """
    from egomimic.utils.egomimicUtils import draw_actions

    H, W = panel_size, panel_size

    # Scale intrinsics from their calibrated reference resolution to panel_size.
    # ARIA_INTRINSICS has cx=320, cy=240 → calibrated for ~640×480.
    # Projecting without rescaling puts all dots outside the 224×224 panel.
    scaled_intrinsics = _scale_intrinsics(camera_transforms.intrinsics, H, W)

    # ── Actions overlay on current primary (mirrors visualize_preds) ─────────
    current_vis = _resize(current_primary, H, W)
    if gt_actions is not None and pred_actions is not None and len(pred_actions) > 0:
        pred_xyz, _ = _extract_xyz(pred_actions)
        gt_xyz,   _ = _extract_xyz(gt_actions)

        d = pred_xyz.shape[-1]
        ac_type = "joints" if d in (7, 14) else "xyz"
        arm     = "right"  if d in (7, 3)  else "both"

        try:
            current_vis = draw_actions(
                current_vis, ac_type, "Purples", pred_xyz,
                camera_transforms.extrinsics, scaled_intrinsics, arm=arm,
            )
            current_vis = draw_actions(
                current_vis, ac_type, "Greens", gt_xyz,
                camera_transforms.extrinsics, scaled_intrinsics, arm=arm,
            )
        except Exception as e:
            logger.warning("draw_actions failed: %s", e)
            import traceback; traceback.print_exc()

    # ── Panel 1: current + actions (blue label) ───────────────────────────────
    p_current  = _labeled(_resize(current_vis,        H, W), "Current + actions", (180, 180, 255))
    p_gt       = _labeled(_resize(gt_future_primary,  H, W), "GT future",         (180, 255, 180))
    p_wm       = _labeled(_resize(wm_future_primary,  H, W), "WM pred",           (255, 180, 180))
    primary_row = np.concatenate([p_current, p_gt, p_wm], axis=1)  # (H, 3W, 3)

    # ── Optional wrist row ────────────────────────────────────────────────────
    if (current_left_wrist   is not None and
        gt_future_left_wrist is not None and
        wm_future_left_wrist is not None):
        pw_cur = _labeled(_resize(current_left_wrist,    H, W), "Wrist current",    (180, 180, 255))
        pw_gt  = _labeled(_resize(gt_future_left_wrist,  H, W), "Wrist GT future",  (180, 255, 180))
        pw_wm  = _labeled(_resize(wm_future_left_wrist,  H, W), "Wrist WM pred",    (255, 180, 180))
        wrist_row = np.concatenate([pw_cur, pw_gt, pw_wm], axis=1)
        return np.concatenate([primary_row, wrist_row], axis=0)   # (2H, 3W, 3)

    return primary_row  # (H, 3W, 3)


# ---------------------------------------------------------------------------
# Episode grouping
# ---------------------------------------------------------------------------

def group_episodes_from_dataset(
    dataset,
    max_episodes: Optional[int] = None,
    max_frames: Optional[int] = None,
) -> dict:
    """
    Scan demo_dataset (S3RLDBDataset) and group item indices by episode_index.

    Stops scanning as soon as max_frames total frames have been collected,
    so --n avoids loading the entire dataset.

    Returns:
        OrderedDict: episode_id (int) → sorted list of (frame_index, global_idx)
    """
    s3_ds   = dataset.demo_dataset
    n_items = len(s3_ds) if max_frames is None else min(len(s3_ds), max_frames)
    episodes = defaultdict(list)

    log_message(f"Scanning {n_items} items into episodes...", None)
    for i in tqdm.tqdm(range(n_items), desc="Scanning episodes"):
        item   = s3_ds[i]
        ep_idx = int(item.get("episode_index", 0))
        fr_idx = int(item.get("frame_index",   i))
        episodes[ep_idx].append((fr_idx, i))

    # Sort each episode by frame_index
    for ep_idx in episodes:
        episodes[ep_idx].sort(key=lambda x: x[0])

    # Optionally cap number of episodes
    ep_ids = sorted(episodes.keys())
    if max_episodes is not None:
        ep_ids = ep_ids[:max_episodes]

    return {ep: episodes[ep] for ep in ep_ids}


# ---------------------------------------------------------------------------
# Per-episode visualizer
# ---------------------------------------------------------------------------

def visualize_episode(
    cfg: PolicyEvalVisualizeConfig,
    episode_idx: int,
    frame_pairs: list,       # list of (frame_index, global_dataset_idx)
    model,
    dataset,
    device: torch.device,
    camera_transforms,
    log_file=None,
) -> str:
    """
    Run inference and produce a visualization MP4 for one episode.

    For each timestep:
      1. dataset[global_idx] → cosmos_batch (already in egomimic format)
      2. model.generate_samples_from_batch → generated_latent
      3. extract_action_chunk_from_latent_sequence → predicted actions
      4. decode_wm_future_images → WM-decoded future images
      5. build_visualization_frame → 3-panel strip (egomimic style)
    """
    s3_ds  = dataset.demo_dataset

    # Infer model dtype once (e.g., bfloat16) so inputs match weights.
    model_dtype: Optional[torch.dtype] = None
    for p in model.parameters():
        if torch.is_floating_point(p):
            model_dtype = p.dtype
            break

    os.makedirs(cfg.save_video_path, exist_ok=True)
    video_path  = os.path.join(
        cfg.save_video_path,
        f"episode_{episode_idx}_{cfg.run_id_note or 'eval'}.mp4",
    )
    writer = imageio.get_writer(video_path, fps=cfg.video_fps)

    log_message(f"Episode {episode_idx}: {len(frame_pairs)} steps → {video_path}", log_file)

    for step, (_, global_idx) in enumerate(tqdm.tqdm(frame_pairs, desc=f"ep {episode_idx}")):
        try:
            # ── 1. Get preprocessed sample (cosmos_batch format) ───────────
            sample = dataset[global_idx]
            cosmos_batch = _batch_sample(sample, device, model_dtype)

            # ── 2. Model inference ─────────────────────────────────────────
            with torch.no_grad():
                generated_latent, orig_clean_latent_frames = (
                    model.generate_samples_from_batch(
                        cosmos_batch,
                        num_steps=cfg.num_denoising_steps,
                        return_orig_clean_latent_frames=True,
                    )
                )

            # ── 3. Extract predicted action chunk ──────────────────────────
            action_shape = (
                int(cosmos_batch["actions"].shape[1]),
                int(cosmos_batch["actions"].shape[2]),
            )
            pred_actions_tensor = extract_action_chunk_from_latent_sequence(
                generated_latent,
                action_shape=action_shape,
                action_indices=cosmos_batch["action_latent_idx"],
            )  # (1, chunk_size, action_dim)
            pred_actions = pred_actions_tensor[0].cpu().numpy()  # (chunk_size, action_dim)

            # GT actions from sample (raw / un-normalized from S3RLDBDataset)
            raw_item = s3_ds[global_idx]
            raw_action = raw_item.get(dataset.action_key)
            if isinstance(raw_action, torch.Tensor):
                raw_action = raw_action.cpu().numpy()
            raw_action = np.asarray(raw_action, dtype=np.float32)
            if raw_action.ndim == 1:
                raw_action = raw_action[np.newaxis, :]
            gt_actions = raw_action[: cfg.chunk_size]  # (chunk_size, action_dim)

            # ── 4. Decode WM future images ─────────────────────────────────
            wm_preds = decode_wm_future_images(
                model, generated_latent, orig_clean_latent_frames, cosmos_batch
            )

            # ── 5. Get raw images for visualization ────────────────────────
            def _raw_img(key):
                v = raw_item.get(key)
                if v is None:
                    return None
                img = _to_uint8(
                    v.cpu().numpy() if isinstance(v, torch.Tensor) else v
                )
                return dataset._ensure_size(img)

            current_primary  = _raw_img(dataset.primary_camera_key)
            gt_future_primary = _raw_img(f"future.{dataset.primary_camera_key}")
            current_left_wrist   = _raw_img(dataset.left_wrist_camera_key)
            gt_future_left_wrist = _raw_img(f"future.{dataset.left_wrist_camera_key}")

            if gt_future_primary is None:
                gt_future_primary = np.zeros_like(current_primary)

            # WM predicted images: (1, H, W, 3) → (H, W, 3)
            wm_primary = (
                wm_preds["future_image_pred"][0] if wm_preds else
                np.zeros_like(current_primary)
            )
            wm_left_wrist = (
                wm_preds.get("future_wrist_image_pred", np.zeros((1,) + current_primary.shape))[0]
                if wm_preds else None
            )

            # ── 6. Build frame (egomimic visualize_preds layout) ───────────
            frame = build_visualization_frame(
                current_primary=current_primary,
                gt_future_primary=gt_future_primary,
                wm_future_primary=wm_primary,
                gt_actions=gt_actions,
                pred_actions=pred_actions,
                camera_transforms=camera_transforms,
                panel_size=cfg.panel_size,
                current_left_wrist=current_left_wrist,
                gt_future_left_wrist=gt_future_left_wrist,
                wm_future_left_wrist=wm_left_wrist,
            )
            writer.append_data(frame)

        except Exception as e:
            logger.warning("Step %d of episode %d failed: %s", step, episode_idx, e)
            import traceback; traceback.print_exc()
            # Write a blank frame so video length stays consistent
            blank = np.zeros((cfg.panel_size * 2, cfg.panel_size * 3, 3), dtype=np.uint8)
            writer.append_data(blank)

    writer.close()
    log_message(f"Saved: {video_path}", log_file)
    return video_path


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

@draccus.wrap()
def eval_egoverse_visualize(cfg: PolicyEvalVisualizeConfig):
    """Visualize EgoVerse model predictions on training dataset episodes."""
    assert cfg.ckpt_path, "ckpt_path must be set"
    assert cfg.config,    "config must be set"

    if cfg.deterministic:
        os.environ["DETERMINISTIC"] = "True"
    set_seed_everywhere(cfg.seed)

    device = DEVICE

    # ── Load model ────────────────────────────────────────────────────────
    log_message("Loading model...", None)
    model, cosmos_config = get_model(cfg)
    model.eval()
    model.to(device)

    # ── Load dataset from config ──────────────────────────────────────────
    log_message("Instantiating EgoVerseDataset from cosmos config...", None)
    dataset = instantiate(cosmos_config.dataloader_train.dataset)

    # Verify chunk size matches
    ds_chunk = getattr(dataset, "chunk_size", cfg.chunk_size)
    assert cfg.chunk_size == ds_chunk, (
        f"chunk_size mismatch: cfg={cfg.chunk_size}, dataset={ds_chunk}"
    )

    # Populate T5 embedding cache from dataset (for cosmos_utils.get_action compatibility)
    if getattr(dataset, "t5_text_embeddings", None) is not None:
        from cosmos_policy.experiments.robot.cosmos_utils import t5_text_embeddings_cache
        for k, v in dataset.t5_text_embeddings.items():
            t5_text_embeddings_cache[k] = (
                v.to(device) if isinstance(v, torch.Tensor) else v
            )
        log_message(f"T5 cache: {len(t5_text_embeddings_cache)} entries", None)

    # ── CameraTransforms (same as egomimic training config) ───────────────
    from egomimic.utils.egomimicUtils import CameraTransforms
    camera_transforms = CameraTransforms(
        intrinsics_key="base",
        extrinsics_key="x5Dec13_2",
    )

    # ── Group items by episode ────────────────────────────────────────────
    all_episodes = group_episodes_from_dataset(
        dataset, max_episodes=cfg.max_episodes, max_frames=cfg.n
    )

    if cfg.episode_indices is not None and cfg.episode_indices.strip():
        selected_keys = [int(x) for x in cfg.episode_indices.split(",")]
        episodes = {k: all_episodes[k] for k in selected_keys if k in all_episodes}
    else:
        episodes = all_episodes

    log_message(f"Visualizing {len(episodes)} episodes", None)

    # ── Setup logging ─────────────────────────────────────────────────────
    log_file, _, _ = setup_logging(
        cfg=cfg,
        task_identifier="visualize",
        log_dir=cfg.local_log_dir,
        run_id_note=cfg.run_id_note,
        use_wandb=False,
        wandb_entity="",
        wandb_project="",
    )

    # ── Visualize ─────────────────────────────────────────────────────────
    os.makedirs(cfg.save_video_path, exist_ok=True)
    for ep_idx, frame_pairs in tqdm.tqdm(episodes.items(), desc="Episodes"):
        visualize_episode(
            cfg=cfg,
            episode_idx=ep_idx,
            frame_pairs=frame_pairs,
            model=model,
            dataset=dataset,
            device=device,
            camera_transforms=camera_transforms,
            log_file=log_file,
        )

    log_message("Done.", log_file)
    if log_file:
        log_file.close()


if __name__ == "__main__":
    eval_egoverse_visualize()
